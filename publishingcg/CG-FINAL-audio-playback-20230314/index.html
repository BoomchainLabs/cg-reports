<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US"><head>
<meta charset="utf-8">
<meta name="generator" content="ReSpec 32.7.1">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<style>
.issue-label{text-transform:initial}
.warning>p:first-child{margin-top:0}
.warning{padding:.5em;border-left-width:.5em;border-left-style:solid}
span.warning{padding:.1em .5em .15em}
.issue.closed span.issue-number{text-decoration:line-through}
.issue.closed span.issue-number::after{content:" (Closed)";font-size:smaller}
.warning{border-color:#f11;border-width:.2em;border-style:solid;background:#fbe9e9}
.warning-title:before{content:"⚠";font-size:1.3em;float:left;padding-right:.3em;margin-top:-.3em}
li.task-list-item{list-style:none}
input.task-list-item-checkbox{margin:0 .35em .25em -1.6em;vertical-align:middle}
.issue a.respec-gh-label{padding:5px;margin:0 2px 0 2px;font-size:10px;text-transform:none;text-decoration:none;font-weight:700;border-radius:4px;position:relative;bottom:2px;border:none;display:inline-block}
</style>
<style>
dfn{cursor:pointer}
.dfn-panel{position:absolute;z-index:35;min-width:300px;max-width:500px;padding:.5em .75em;margin-top:.6em;font-family:"Helvetica Neue",sans-serif;font-size:small;background:#fff;color:#000;box-shadow:0 1em 3em -.4em rgba(0,0,0,.3),0 0 1px 1px rgba(0,0,0,.05);border-radius:2px}
.dfn-panel:not(.docked)>.caret{position:absolute;top:-9px}
.dfn-panel:not(.docked)>.caret::after,.dfn-panel:not(.docked)>.caret::before{content:"";position:absolute;border:10px solid transparent;border-top:0;border-bottom:10px solid #fff;top:0}
.dfn-panel:not(.docked)>.caret::before{border-bottom:9px solid #a2a9b1}
.dfn-panel *{margin:0}
.dfn-panel b{display:block;color:#000;margin-top:.25em}
.dfn-panel ul a[href]{color:#333}
.dfn-panel>div{display:flex}
.dfn-panel a.self-link{font-weight:700;margin-right:auto}
.dfn-panel .marker{padding:.1em;margin-left:.5em;border-radius:.2em;text-align:center;white-space:nowrap;font-size:90%;color:#040b1c}
.dfn-panel .marker.dfn-exported{background:#d1edfd;box-shadow:0 0 0 .125em #1ca5f940}
.dfn-panel .marker.idl-block{background:#8ccbf2;box-shadow:0 0 0 .125em #0670b161}
.dfn-panel a:not(:hover){text-decoration:none!important;border-bottom:none!important}
.dfn-panel a[href]:hover{border-bottom-width:1px}
.dfn-panel ul{padding:0}
.dfn-panel li{margin-left:1em}
.dfn-panel.docked{position:fixed;left:.5em;top:unset;bottom:2em;margin:0 auto;max-width:calc(100vw - .75em * 2 - .5em - .2em * 2);max-height:30vh;overflow:auto}
</style>
		
		
<title>Publishing Guide to Audio Playback and Text-To-Speech</title>
		
		
	
<style id="respec-mainstyle">
@keyframes pop{
0%{transform:scale(1,1)}
25%{transform:scale(1.25,1.25);opacity:.75}
100%{transform:scale(1,1)}
}
:is(h1,h2,h3,h4,h5,h6,a) abbr{border:none}
dfn{font-weight:700}
a.internalDFN{color:inherit;border-bottom:1px solid #99c;text-decoration:none}
a.externalDFN{color:inherit;border-bottom:1px dotted #ccc;text-decoration:none}
a.bibref{text-decoration:none}
.respec-offending-element:target{animation:pop .25s ease-in-out 0s 1}
.respec-offending-element,a[href].respec-offending-element{text-decoration:red wavy underline}
@supports not (text-decoration:red wavy underline){
.respec-offending-element:not(pre){display:inline-block}
.respec-offending-element{background:url(data:image/gif;base64,R0lGODdhBAADAPEAANv///8AAP///wAAACwAAAAABAADAEACBZQjmIAFADs=) bottom repeat-x}
}
#references :target{background:#eaf3ff;animation:pop .4s ease-in-out 0s 1}
cite .bibref{font-style:normal}
a[href].orcid{padding-left:4px;padding-right:4px}
a[href].orcid>svg{margin-bottom:-2px}
.toc a,.tof a{text-decoration:none}
a .figno,a .secno{color:#000}
ol.tof,ul.tof{list-style:none outside none}
.caption{margin-top:.5em;font-style:italic}
table.simple{border-spacing:0;border-collapse:collapse;border-bottom:3px solid #005a9c}
.simple th{background:#005a9c;color:#fff;padding:3px 5px;text-align:left}
.simple th a{color:#fff;padding:3px 5px;text-align:left}
.simple th[scope=row]{background:inherit;color:inherit;border-top:1px solid #ddd}
.simple td{padding:3px 10px;border-top:1px solid #ddd}
.simple tr:nth-child(even){background:#f0f6ff}
.section dd>p:first-child{margin-top:0}
.section dd>p:last-child{margin-bottom:0}
.section dd{margin-bottom:1em}
.section dl.attrs dd,.section dl.eldef dd{margin-bottom:0}
#issue-summary>ul{column-count:2}
#issue-summary li{list-style:none;display:inline-block}
details.respec-tests-details{margin-left:1em;display:inline-block;vertical-align:top}
details.respec-tests-details>*{padding-right:2em}
details.respec-tests-details[open]{z-index:999999;position:absolute;border:thin solid #cad3e2;border-radius:.3em;background-color:#fff;padding-bottom:.5em}
details.respec-tests-details[open]>summary{border-bottom:thin solid #cad3e2;padding-left:1em;margin-bottom:1em;line-height:2em}
details.respec-tests-details>ul{width:100%;margin-top:-.3em}
details.respec-tests-details>li{padding-left:1em}
.self-link:hover{opacity:1;text-decoration:none;background-color:transparent}
aside.example .marker>a.self-link{color:inherit}
.header-wrapper{display:flex;align-items:baseline}
:is(h2,h3,h4,h5,h6):not(#toc>h2,#abstract>h2,#sotd>h2,.head>h2){position:relative;left:-.5em}
:is(h2,h3,h4,h5,h6):not(#toch2)+a.self-link{color:inherit;order:-1;position:relative;left:-1.1em;font-size:1rem;opacity:.5}
:is(h2,h3,h4,h5,h6)+a.self-link::before{content:"§";text-decoration:none;color:var(--heading-text)}
:is(h2,h3)+a.self-link{top:-.2em}
:is(h4,h5,h6)+a.self-link::before{color:#000}
@media (max-width:767px){
dd{margin-left:0}
}
@media print{
.removeOnSave{display:none}
}
</style>
<meta name="description" content="This guide provides a brief introduction to the different types of audio playback typically found in
				publications. It covers the differences between the types of audio publications created by publishers
				and audio playback automatically generated by user agents. It also explains how screen readers and other
				Assistive Technologies (AT) produce audio through Text-to-Speech (TTS), which often blurs the lines
				between a prepared title designed for audio consumption and a text-based title that readers can consume
				using audio through Assistive Technologies.">
<link rel="canonical" href="https://www.w3.org/publishing/a11y/audio-playback/">
<style>
var{position:relative;cursor:pointer}
var[data-type]::after,var[data-type]::before{position:absolute;left:50%;top:-6px;opacity:0;transition:opacity .4s;pointer-events:none}
var[data-type]::before{content:"";transform:translateX(-50%);border-width:4px 6px 0 6px;border-style:solid;border-color:transparent;border-top-color:#000}
var[data-type]::after{content:attr(data-type);transform:translateX(-50%) translateY(-100%);background:#000;text-align:center;font-family:"Dank Mono","Fira Code",monospace;font-style:normal;padding:6px;border-radius:3px;color:#daca88;text-indent:0;font-weight:400}
var[data-type]:hover::after,var[data-type]:hover::before{opacity:1}
</style>
<script id="initialUserConfig" type="application/json">{
  "group": "publishingcg",
  "edDraftURI": "https://w3c.github.io/publ-a11y/drafts/audio-playback/",
  "latestVersion": "https://www.w3.org/publishing/a11y/audio-playback/",
  "specStatus": "CG-FINAL",
  "shortName": "audio-playback",
  "noRecTrack": true,
  "copyrightStart": "2022",
  "editors": [
    {
      "name": "George Kerscher",
      "company": "DAISY Consortium",
      "companyURL": "http://daisy.org",
      "w3cid": 1460
    },
    {
      "name": "Matt Garrish",
      "company": "DAISY Consortium",
      "companyURL": "http://daisy.org",
      "w3cid": 51655
    }
  ],
  "processVersion": 2020,
  "includePermalinks": true,
  "permalinkEdge": true,
  "permalinkHide": false,
  "github": {
    "repoURL": "https://github.com/w3c/publ-a11y",
    "branch": "main"
  },
  "publishDate": "2023-03-14",
  "publishISODate": "2023-03-14T00:00:00.000Z",
  "generatedSubtitle": "Final Community Group Report 14 March 2023"
}</script>
<link rel="stylesheet" href="https://www.w3.org/StyleSheets/TR/2021/cg-final"></head>
	<body class="h-entry informative"><div class="head">
    
    <h1 id="title" class="title">Publishing Guide to Audio Playback and Text-To-Speech</h1> 
    <p id="w3c-state">
      <a href="https://www.w3.org/standards/types#reports">Final Community Group Report</a>
      <time class="dt-published" datetime="2023-03-14">14 March 2023</time>
    </p>
    <dl>
      <dt>This version:</dt><dd>
              <a class="u-url" href="https://www.w3.org/community/reports/publishingcg/CG-FINAL-audio-playback-20230314/">https://www.w3.org/community/reports/publishingcg/CG-FINAL-audio-playback-20230314/</a>
            </dd>
      <dt>Latest published version:</dt><dd>
              <a href="https://www.w3.org/publishing/a11y/audio-playback/">https://www.w3.org/publishing/a11y/audio-playback/</a>
            </dd>
      <dt>Latest editor's draft:</dt><dd><a href="https://w3c.github.io/publ-a11y/drafts/audio-playback/">https://w3c.github.io/publ-a11y/drafts/audio-playback/</a></dd>
      
      
      
      
      <dt>Editors:</dt><dd class="editor p-author h-card vcard" data-editor-id="1460">
    <span class="p-name fn">George Kerscher</span> (<a class="p-org org h-org" href="http://daisy.org">DAISY Consortium</a>)
  </dd><dd class="editor p-author h-card vcard" data-editor-id="51655">
    <span class="p-name fn">Matt Garrish</span> (<a class="p-org org h-org" href="http://daisy.org">DAISY Consortium</a>)
  </dd>
      
      
      <dt>Feedback:</dt><dd>
        <a href="https://github.com/w3c/publ-a11y/">GitHub w3c/publ-a11y</a>
        (<a href="https://github.com/w3c/publ-a11y/pulls/">pull requests</a>,
        <a href="https://github.com/w3c/publ-a11y/issues/new/choose">new issue</a>,
        <a href="https://github.com/w3c/publ-a11y/issues/">open issues</a>)
      </dd>
    </dl>
    
    <p class="copyright">
          <a href="https://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a>
          ©
          2022-2023
          
          the Contributors to the Publishing Guide to Audio Playback and Text-To-Speech
          Specification, published by the
          <a href="https://www.w3.org/groups/cg/publishingcg">Publishing Community Group</a> under the
          <a href="https://www.w3.org/community/about/agreements/fsa/">W3C Community Final Specification Agreement (FSA)</a>. A human-readable
                <a href="https://www.w3.org/community/about/agreements/fsa-deed/">summary</a>
                is available.
              
        </p>
    <hr title="Separator for header">
  </div>
		<section id="abstract" class="introductory"><h2>Abstract</h2>
			<p>This guide provides a brief introduction to the different types of audio playback typically found in
				publications. It covers the differences between the types of audio publications created by publishers
				and audio playback automatically generated by user agents. It also explains how screen readers and other
				Assistive Technologies (AT) produce audio through Text-to-Speech (TTS), which often blurs the lines
				between a prepared title designed for audio consumption and a text-based title that readers can consume
				using audio through Assistive Technologies.</p>
		</section>
		<section id="sotd" class="introductory"><h2>Status of This Document</h2><p>
      This specification was published by the
      <a href="https://www.w3.org/groups/cg/publishingcg">Publishing Community Group</a>. It is not a W3C Standard nor is it
      on the W3C Standards Track.
      
            Please note that under the
            <a href="https://www.w3.org/community/about/agreements/final/">W3C Community Final Specification Agreement (FSA)</a>
            other conditions apply.
          
      Learn more about
      <a href="https://www.w3.org/community/">W3C Community and Business Groups</a>.
    </p><p>
    <a href="https://github.com/w3c/publ-a11y/issues/">GitHub Issues</a> are preferred for
          discussion of this specification.
        
    
  </p></section><nav id="toc"><h2 class="introductory" id="table-of-contents">Table of Contents</h2><ol class="toc"><li class="tocline"><a class="tocxref" href="#abstract">Abstract</a></li><li class="tocline"><a class="tocxref" href="#sotd">Status of This Document</a></li><li class="tocline"><a class="tocxref" href="#overview"><bdi class="secno">1. </bdi>Overview</a></li><li class="tocline"><a class="tocxref" href="#audiobooks"><bdi class="secno">2. </bdi>Audiobooks</a></li><li class="tocline"><a class="tocxref" href="#full-audio"><bdi class="secno">3. </bdi>Full Audio Publications</a></li><li class="tocline"><a class="tocxref" href="#tts"><bdi class="secno">4. </bdi>Text-To-Speech Synthesis</a></li></ol></nav>
		<section id="overview"><div class="header-wrapper"><h2 id="x1-overview"><bdi class="secno">1. </bdi>Overview</h2><a class="self-link" href="#overview" aria-label="Permalink for Section 1."></a></div>
			

			<p>Publishing has a sometimes-dizzying array of ways of referring to audio playback — audiobook,
				talking book, read aloud book, text-to-speech playback, media overlays and full audio are some of the
				most common terms.</p>

			<p>Compounding the confusion is that these are often all referred to as audio books (two words), even though
				there is a unique form of publication called an audiobook (one word). Moreover, Text-To-Speech synthesis
				is often talked about as though it is an audio format even though it is a feature of the device or the
				Assistive Technology a reader is using.</p>

			<p>Despite the many terms used to describe audio playback, there are only two primary models for classifying
				publications with recorded audio:</p>

			<dl>
				<dt>Audiobook</dt>
				<dd>
					<p>A publication whose primary, and typically only, way of being read is auditorily.</p>
				</dd>

				<dt>Full Audio</dt>
				<dd>
					<p>The content of a publication is available in auditory form, but the audio is often structured
						together with textual or visual content.</p>
				</dd>
			</dl>

			<p>Slightly tangential to these models, but equally important, is Text-To-Speech synthesis. Speech synthesis
				is not technically a form of audio playback but a way to have the device a reader is using synthetically
				voice the text content of a publication. Speech synthesis is like an on-demand rendering of content in
				this way, but due to the limitations of text-to-speech rendering engines the result is often not as
				precise and clear as professional human narration.</p>

			<p>The rest of this guide explores each of these technologies in more detail and demystifies the terminology
				used to describe them.</p>
		</section>
		<section id="audiobooks"><div class="header-wrapper"><h2 id="x2-audiobooks"><bdi class="secno">2. </bdi>Audiobooks</h2><a class="self-link" href="#audiobooks" aria-label="Permalink for Section 2."></a></div>
			

			<p>An audiobook provides prerecorded narration of a work. Publishers typically structure audiobooks as a
				series of one or more audio files that readers will listen to in sequence.</p>

			<p>Readers may stream the audio directly from an audiobook publisher, like Audible, or may obtain a set of
				audio files that they can play back on any device or application with audio playback capabilities.</p>

			<p>The key feature of an audiobook is that it is designed to be listened to. If there is any text content,
				it is typically quite minimal (e.g., a playlist to aid in playback on specific devices or a table of
				contents).</p>

			<p>A recent W3C standard, aptly named <a href="https://www.w3.org/TR/audiobooks/">Audiobooks</a>, seeks to
				bring greater structure to the world of audiobooks by introducing a formal syntax for describing
				audiobook metadata, listing the resources of the book, and defining a play order, among other
				features.</p>

			<div class="note" role="note" id="issue-container-generatedID"><div role="heading" class="note-title marker" id="h-note" aria-level="3"><span>Note</span></div><p class="">Although audiobooks have historically been referred to as talking books, the term "talking
				book" is now more commonly associated with DAISY Digital Talking Books. These are a hybrid between
				audiobooks and full audio publications that are designed for readers who are blind, have low vision, or
				other print disabilities, such as dyslexia.</p></div>
		</section>
		<section id="full-audio"><div class="header-wrapper"><h2 id="x3-full-audio-publications"><bdi class="secno">3. </bdi>Full Audio Publications</h2><a class="self-link" href="#full-audio" aria-label="Permalink for Section 3."></a></div>
			

			<p>A publication with full audio differs from an audiobook in that formats that allow full audio content
				also allow the inclusion of, and synchronization with, the full text content (even if the full text is
				not always available).</p>

			<p>EPUB is an example of a format that allows publishers to include full audio. Although EPUB is a
				text-first format, it includes a technology called Media Overlays that allows publishers to synchronize
				audio with the text for automatic playback.</p>

			<p>Some Reading Systems will omit the text, making it appear as though an EPUB is an audiobook, but, unlike
				audiobooks, there is always a minimal amount of text that publishers must provide. Publications with
				full audio are never as simple to load and play as pure audiobooks are because there are control files
				that a simple audio playback device does not understand.</p>

			<p>Full audio publications are often referred to as "<dfn id="dfn-read-aloud-books" tabindex="0" aria-haspopup="dialog" data-dfn-type="dfn">read aloud books</dfn>" because publishers
				commonly use synchronized text and audio playback in children's works. Children can follow the text as
				the Reading System plays back the audio.</p>
		</section>
		<section id="tts"><div class="header-wrapper"><h2 id="x4-text-to-speech-synthesis"><bdi class="secno">4. </bdi>Text-To-Speech Synthesis</h2><a class="self-link" href="#tts" aria-label="Permalink for Section 4."></a></div>
			

			<p>Text-To-Speech (TTS) synthesis is a form of audio rendering typically produced on demand by a Reading
				System or Assistive Technology. VoiceOver on Apple devices and Talkback on Android are a couple of the
				more commonly known examples of TTS engines built into mainstream phones and tablets, while Jaws and
				NVDA are examples of Assistive Technologies that can translate text to speech for users who need an
				auditory interface to their Windows computers.</p>

			<p>Text-To-Speech synthesis is commonly associated with users who are blind, but many readers benefit from
				being able to render text content auditorily using Text-To-Speech synthesis. Even sighted readers will
				turn to speech synthesis when it is not conducive to read content visually (e.g., when in a moving
				vehicle). Many Reading Systems are now providing TTS rendering as a function in the application. </p>

			<p>Unlike audiobooks and publications with full audio, however, Text-To-Speech synthesis is not an audio
				format. It is a way of listening to text content the reader has already obtained. Readers can use a TTS
				application to read their EPUB publications.</p>

			<div class="note" role="note" id="issue-container-generatedID-0"><div role="heading" class="note-title marker" id="h-note-0" aria-level="3"><span>Note</span></div><div class="">
				<p>The names some reading systems give their Text-to-Speech playback feature can be confusingly similar
					to "<a href="#dfn-read-aloud-books" class="internalDFN" data-link-type="dfn" id="ref-for-dfn-read-aloud-books-1">read aloud books</a>". For example, a button named "Read Now" might initiate Text-to-Speech
					playback. The distinguishing feature between the two is that Text-to-Speech playback employs
					on-the-fly voice synthesizing.</p>
			</div></div>

			<p>The applications that provide Text-To-Speech synthesis for persons with disabilities are usually not
				specifically designed for reading publications, however. They are general tools that aid navigation
				across the device the reader is using and any other apps and content that are on it. As a result, they
				often only have a very limited built-in vocabulary of pronunciations so are not able to provide a
				similar quality of playback as prerecorded human narration.</p>

			<p>When a user enables Text-To-Speech playback, the Reading System or Assistive Technology feeds the text
				content of the publication to an underlying TTS engine that voices each word. For most general language,
				the rendering returned is reasonably good, but the engines will struggle with works that contain complex
				terms, jargon, uncommon names, etc. Readers consequently also typically have the option to have
				individual words spelled out, both to help disambiguate similar-sounding words and to make sense of
				complex words and heteronyms (words spelled the same but with different pronunciations) that
				Text-to-Speech engines mispronounce. This ability to explore the spelling of words, which readers cannot
				do with human narrated titles, is a particular advantage of Text-To-Speech synthesis. </p>

			<p>Although Text-To-Speech synthesis is not audio playback provided by the publisher, it is sometimes
				possible for publishers to help improve the quality of the rendering. Technologies such as the <a href="https://www.w3.org/TR/speech-synthesis11/">Speech Synthesis Markup Language (SSML)</a> and <a href="https://www.w3.org/TR/pronunciation-lexicon/">Pronunciation Lexicon Specification (PLS)</a>
				allow publishers to provide the proper phonetic pronunciation of complex words and heteronyms.
				Unfortunately, support for these technologies is not yet widespread.</p>

			<p>Text-To-Speech synthesis provides a reasonable alternative to audio playback formats and allows readers
				to speed up playback to high rates. In addition, readers can also synchronize Text-To-Speech playback
				with refreshable braille displays. However, Text-To-Speech remains an imperfect means of reading
				publications, especially when compared to the wonderful talented narrators that produce audio books.</p>
		</section>
	

<p role="navigation" id="back-to-top">
    <a href="#title"><abbr title="Back to Top">↑</abbr></a>
  </p><div class="dfn-panel" hidden="" role="dialog" aria-modal="true" id="dfn-panel-for-dfn-read-aloud-books" aria-label="Links in this document to definition: read aloud books">
      <span class="caret"></span>
      <div>
        <a class="self-link" href="#dfn-read-aloud-books" aria-label="Permalink for definition: read aloud books. Activate to close this dialog.">Permalink</a>
         
      </div>
      <p><b>Referenced in:</b></p>
      <ul>
    <li>
    <a href="#ref-for-dfn-read-aloud-books-1" title="§ 4. Text-To-Speech Synthesis">§ 4. Text-To-Speech Synthesis</a> 
  </li>
  </ul>
    </div><script id="respec-dfn-panel">(() => {
// @ts-check
if (document.respec) {
  document.respec.ready.then(setupPanel);
} else {
  setupPanel();
}

function setupPanel() {
  const listener = panelListener();
  document.body.addEventListener("keydown", listener);
  document.body.addEventListener("click", listener);
}

function panelListener() {
  /** @type {HTMLElement} */
  let panel = null;
  return event => {
    const { target, type } = event;

    if (!(target instanceof HTMLElement)) return;

    // For keys, we only care about Enter key to activate the panel
    // otherwise it's activated via a click.
    if (type === "keydown" && event.key !== "Enter") return;

    const action = deriveAction(event);

    switch (action) {
      case "show": {
        hidePanel(panel);
        /** @type {HTMLElement} */
        const dfn = target.closest("dfn, .index-term");
        panel = document.getElementById(`dfn-panel-for-${dfn.id}`);
        const coords = deriveCoordinates(event);
        displayPanel(dfn, panel, coords);
        break;
      }
      case "dock": {
        panel.style.left = null;
        panel.style.top = null;
        panel.classList.add("docked");
        break;
      }
      case "hide": {
        hidePanel(panel);
        panel = null;
        break;
      }
    }
  };
}

/**
 * @param {MouseEvent|KeyboardEvent} event
 */
function deriveCoordinates(event) {
  const target = /** @type HTMLElement */ (event.target);

  // We prevent synthetic AT clicks from putting
  // the dialog in a weird place. The AT events sometimes
  // lack coordinates, so they have clientX/Y = 0
  const rect = target.getBoundingClientRect();
  if (
    event instanceof MouseEvent &&
    event.clientX >= rect.left &&
    event.clientY >= rect.top
  ) {
    // The event probably happened inside the bounding rect...
    return { x: event.clientX, y: event.clientY };
  }

  // Offset to the middle of the element
  const x = rect.x + rect.width / 2;
  // Placed at the bottom of the element
  const y = rect.y + rect.height;
  return { x, y };
}

/**
 * @param {Event} event
 */
function deriveAction(event) {
  const target = /** @type {HTMLElement} */ (event.target);
  const hitALink = !!target.closest("a");
  if (target.closest("dfn:not([data-cite]), .index-term")) {
    return hitALink ? "none" : "show";
  }
  if (target.closest(".dfn-panel")) {
    if (hitALink) {
      return target.classList.contains("self-link") ? "hide" : "dock";
    }
    const panel = target.closest(".dfn-panel");
    return panel.classList.contains("docked") ? "hide" : "none";
  }
  if (document.querySelector(".dfn-panel:not([hidden])")) {
    return "hide";
  }
  return "none";
}

/**
 * @param {HTMLElement} dfn
 * @param {HTMLElement} panel
 * @param {{ x: number, y: number }} clickPosition
 */
function displayPanel(dfn, panel, { x, y }) {
  panel.hidden = false;
  // distance (px) between edge of panel and the pointing triangle (caret)
  const MARGIN = 20;

  const dfnRects = dfn.getClientRects();
  // Find the `top` offset when the `dfn` can be spread across multiple lines
  let closestTop = 0;
  let minDiff = Infinity;
  for (const rect of dfnRects) {
    const { top, bottom } = rect;
    const diffFromClickY = Math.abs((top + bottom) / 2 - y);
    if (diffFromClickY < minDiff) {
      minDiff = diffFromClickY;
      closestTop = top;
    }
  }

  const top = window.scrollY + closestTop + dfnRects[0].height;
  const left = x - MARGIN;
  panel.style.left = `${left}px`;
  panel.style.top = `${top}px`;

  // Find if the panel is flowing out of the window
  const panelRect = panel.getBoundingClientRect();
  const SCREEN_WIDTH = Math.min(window.innerWidth, window.screen.width);
  if (panelRect.right > SCREEN_WIDTH) {
    const newLeft = Math.max(MARGIN, x + MARGIN - panelRect.width);
    const newCaretOffset = left - newLeft;
    panel.style.left = `${newLeft}px`;
    /** @type {HTMLElement} */
    const caret = panel.querySelector(".caret");
    caret.style.left = `${newCaretOffset}px`;
  }

  // As it's a dialog, we trap focus.
  // TODO: when <dialog> becomes a implemented, we should really
  // use that.
  trapFocus(panel, dfn);
}

/**
 * @param {HTMLElement} panel
 * @param {HTMLElement} dfn
 * @returns
 */
function trapFocus(panel, dfn) {
  /** @type NodeListOf<HTMLAnchorElement> elements */
  const anchors = panel.querySelectorAll("a[href]");
  // No need to trap focus
  if (!anchors.length) return;

  // Move focus to first anchor element
  const first = anchors.item(0);
  first.focus();

  const trapListener = createTrapListener(anchors, panel, dfn);
  panel.addEventListener("keydown", trapListener);

  // Hiding the panel releases the trap
  const mo = new MutationObserver(records => {
    const [record] = records;
    const target = /** @type HTMLElement */ (record.target);
    if (target.hidden) {
      panel.removeEventListener("keydown", trapListener);
      mo.disconnect();
    }
  });
  mo.observe(panel, { attributes: true, attributeFilter: ["hidden"] });
}

/**
 *
 * @param {NodeListOf<HTMLAnchorElement>} anchors
 * @param {HTMLElement} panel
 * @param {HTMLElement} dfn
 * @returns
 */
function createTrapListener(anchors, panel, dfn) {
  const lastIndex = anchors.length - 1;
  let currentIndex = 0;
  return event => {
    switch (event.key) {
      // Hitting "Tab" traps us in a nice loop around elements.
      case "Tab": {
        event.preventDefault();
        currentIndex += event.shiftKey ? -1 : +1;
        if (currentIndex < 0) {
          currentIndex = lastIndex;
        } else if (currentIndex > lastIndex) {
          currentIndex = 0;
        }
        anchors.item(currentIndex).focus();
        break;
      }

      // Hitting "Enter" on an anchor releases the trap.
      case "Enter":
        hidePanel(panel);
        break;

      // Hitting "Escape" returns focus to dfn.
      case "Escape":
        hidePanel(panel);
        dfn.focus();
        return;
    }
  };
}

/** @param {HTMLElement} panel */
function hidePanel(panel) {
  if (!panel) return;
  panel.hidden = true;
  panel.classList.remove("docked");
}
})()</script><script src="https://www.w3.org/scripts/TR/2021/fixup.js"></script></body></html>